# DP-203 : Data Engineering on Microsoft Azure
Microsoft 認定資格「Microsoft Certified : Azure Data Engineer Associate」の対象試験です。
- Exam DP-203: Data Engineering on Microsoft Azure (beta) - Learn | Microsoft Docs  
https://docs.microsoft.com/ja-jp/learn/certifications/exams/DP-203

## 評価されるスキルについて (意訳)
**※2023/08/24 に英語版が改定 (アップデート) される予定です。**
Exam DP-203: Data Engineering on Microsoft Azure–Skills Measured  
https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4MbYT

この試験の対象者は、さまざまな構造化、非構造化、およびストリーミング データ システムからのデータを分析ソリューションを構築するための適切なスキーマに統合、変換、および統合するための専門知識を持っている必要があります。  
(Candidates for this exam should have subject matter expertise in integrating, transforming, and consolidating data from various structured, unstructured, and streaming data systems into a suitable schema for building analytics solutions.)

また、利害関係者が探索を通じてデータを理解できるように支援し、さまざまなツールと手法を用いて、安全で準拠したデータ処理パイプラインを構築およびメンテナンスする必要があります。さらに、さまざまな Azure データ サービスとフレームワークを用いて、分析用にクレンジングおよび強化されたデータセットを保存および生成します。このデータ ストアは、最新のデータ ウェアハウス (MDW)、ビッグ データ、レイクハウス アーキテクチャなど、ビジネス要件に基づいてさまざまなアーキテクチャ パターンで設計することが可能です。  
(Azure data engineers help stakeholders understand the data through exploration, and they build and maintain secure and compliant data processing pipelines by using different tools and techniques. These professionals use various Azure data services and frameworks to store and produce cleansed and enhanced datasets for analysis. This data store can be designed with different architecture patterns based on business requirements, including modern data warehouse (MDW), big data, or lakehouse architecture.)

加えて、一連のビジネス要件と制約を考慮し、データ パイプラインとデータ ストアの運用化が高性能/効率的/組織的/信頼できるものであることを保証するのにも役立ちます。運用およびデータ品質の問題を特定してトラブルシューティングするのに役立ちます。また、データ パイプラインを満たすようにデータ プラットフォームを設計、実装、監視、最適化します。  
(Azure data engineers also help to ensure that the operationalization of data pipelines and data stores are high-performing, efficient, organized, and reliable, given a set of business requirements and constraints. These professionals help to identify and troubleshoot operational and data quality issues. They also design, implement, monitor, and optimize data platforms to meet the data pipelines.)

最後に、SQL、Python、Scala などのデータ処理言語に関する確かな知識が必要であり、並列処理とデータ アーキテクチャ パターンを理解する必要があります。Azure Data Factory、Azure Synapse Analytics、Azure Stream Analytics、Azure Event Hubs、Azure Data Lake Storage、Azure Databricks を用いてデータ処理ソリューションを作成することに習熟している必要があります。  
(Candidates for this exam must have solid knowledge of data processing languages, including SQL, Python, and Scala, and they need to understand parallel processing and data architecture patterns. They should be proficient in using Azure Data Factory, Azure Synapse Analytics, Azure Stream Analytics, Azure Event Hubs, Azure Data Lake Storage, and Azure Databricks to create data processing solutions.)

### 対象となる領域 : 試験で評価されるスキル
試験は定期的にアップデートされ、役割を果たすために必要なスキルが反映されます。試験を受ける時期に応じて、スキル測定目標の 2 つのバージョンが含まれています。  
(Our exams are updated periodically to reflect skills that are required to perform a role. We have included two versions of the Skills Measured objectives depending on when you are taking the exam.)

常に、試験の英語版を最初にアップデートします。一部の試験は他の言語にローカライズされており、英語版がアップデートされてから約 8 週間後にアップデートされます。その他の利用可能な言語は、試験の詳細 Web ページの試験のスケジュールセクションに一覧表示されています。ご希望の言語で試験を利用できない場合は、試験を完了するために 30 分の追加時間をリクエストすることが可能です。  
(We always update the English language version of the exam first. Some exams are localized into other languages, and those are updated approximately eight weeks after the English version is updated. Other available languages are listed in the Schedule Exam section of the Exam Details webpage. If the exam isn't available in your preferred language, you can request an additional 30 minutes to complete the exam.)

注 : 評価される各スキル続いて記載している箇条書きの一覧は、スキルの評価項目 (どのように評価しているかを示す) となります。関連するトピックが試験内でカバーされている場合があります。  
(Note : The bullets that follow each of the skills measured are intended to illustrate how we are assessing that skill. Related topics may be covered in the exam.)

注 : ほとんどの問題は、GA (General Availability) となった機能に関するものです。プレビュー機能が一般的に使用されている場合、それらに関する問題が含まれる場合があります。  
(Note : Most questions cover features that are general availability (GA). The exam may contain questions on Preview features if those features are commonly used.)

### 機能グループ
#### データ ストレージの設計と実装 (Design and implement data storage) (15-20%)
- パーティション戦略を実装する  
  (Implement a partition strategy)
  - ファイルのパーティション戦略を実装する  
    (Implement a partition strategy for files)
  - 分析ワークロードのパーティション戦略を実装する  
    (Implement a partition strategy for analytical workloads)
  - ストリーミング ワークロードのパーティション戦略を実装する  
    (Implement a partition strategy for streaming workloads)
  - Azure Synapse Analytics のパーティション戦略を実装する  
    (Implement a partition strategy for Azure Synapse Analytics)
  - Azure Data Lake Storage Gen2 がどのタイミングで必要なパーティショニングを特定する  
    (Identify when partitioning is needed in Azure Data Lake Storage Gen2)
- データ探索レイヤーを設計、実装する  
  (Design and implement the data exploration layer)
  - SQL サーバーレスと Spark クラスターを活用するコンピューティング ソリューションを用いて、クエリを作成、実行する  
    (Create and execute queries by using a compute solution that leverages SQL serverless and Spark cluster)
  - Azure Synapse Analytics データベース テンプレートを実装する  
    (Implement Azure Synapse Analytics database templates)
  - Azure Synapse Analytics データベース テンプレートを推奨する  
    (Recommend Azure Synapse Analytics database templates)
  - 新規/アップデートされたデータ系統を Microsoft Purview にプッシュする  
    (Push new or updated data lineage to Microsoft Purview)
  - Microsoft Purview Data Catalog でメタデータを参照、検索する  
    (Browse and search metadata in Microsoft Purview Data Catalog)
#### データ処理の開発 (Develop data processing) (40-45%)
- データの取り込み、変換する  
  (Ingest and transform data)
  - 増分負荷を設計、実装する
    (Design and implement incremental loads)
  - Apache Spark を用いてデータ変換する  
    (Transform data by using Apache Spark)
  - Transact-SQL (T-SQL) を用いてデータ変換する  
    (Transform data by using Transact-SQL (T-SQL))
  - Azure Synapse Pipelines/Azure Data Factory を用いて、データを取り込み、変換する  
    (Ingest and transform data by using Azure Synapse Pipelines or Azure Data Factory)
  - Azure Stream Analytics を用いてデータ変換する  
    (Transform data by using Azure Stream Analytics)
  - データをクレンジングする  
    (Cleanse data)
  - 重複データを処理する    
    (Handle duplicate data)
  - 欠落データを処理する  
    (Handle missing data)
  - 遅れて到着したデータを処理する  
    (Handle late-arriving data)
  - データを分割する  
    (Split data)
  - JSON を細分化する  
    (Shred JSON)
  - データをエンコード/でコードする  
    (Encode and decode data)
  - 変換におけるエラー ハンドリングを構成する  
    (Configure error handling for a transformation)
  - 値を正規化/非正規化する  
    (Normalize and denormalize values)
  - データ探索分析を実行する  
    (Perform data exploratory analysis)
- バッチ処理ソリューションを開発する  
  (Develop a batch processing solution)
  - Azure Data Lake Storage、Azure Databricks、Azure Synapse Analytics、および Azure Data Factory を用いて、バッチ処理ソリューションを開発する  
    (Develop batch processing solutions by using Azure Data Lake Storage, Azure Databricks, Azure Synapse Analytics, and Azure Data Factory)
  - PolyBase を用いて、データを SQL プールに読み込む  
    (Use PolyBase to load data to a SQL pool)
  - Azure Synapse Link を実装し、レプリケートされたデータに対してクエリを実行する  
    (Implement Azure Synapse Link and query the replicated data)
  - データ パイプラインを作成する  
    (Create data pipelines)
  - リソースをスケールする  
    (Scale resources)
  - バッチ サイズを構成する  
    (Configure the batch size)
  - データ パイプラインのテストを作成する  
    (Create tests for data pipelines)
  - データ パイプラインに Jupyter/Python ノートブックを統合する  
    (Integrate Jupyter or Python notebooks into a data pipeline)
  - データを upsert する  
    (Upsert data)
  - データを以前の状態に戻す  
    (Revert data to a previous state)
  - 例外ハンドリングを構成する  
    (Configure exception handling)
  - バッチ保持を構成する  
    (Configure batch retention)
  - delta lake の読み取り/書き込み  
    (Read from and write to a delta lake)
- ストリーム処理ソリューションを開発する  
  (Develop a stream processing solution)
  - Stream Analytics、および Azure Event Hubs を用いて、ストリーム処理ソリューションを作成する  
    (Create a stream processing solution by using Stream Analytics and Azure Event Hubs)
  - Spark 構造化ストリーミングを用いてデータを処理する  
    (Process data by using Spark structured streaming)
  - ウィンドウ化された集計を作成する  
    (ウィンドウ集計の作成)
  - スキーマ ドリフトをハンドリングする  
    (Handle schema drift)
  - タイム シリーズ データを処理する  
    (Process time series data)
  - パーティション間でデータを処理する  
    (Process data across partitions)
  - 1 つのパーティション内で処理する  
    (Process within one partition)
  - 処理中のチェックポイント、および透かしを構成する  
    (処理中にチェックポイントと透かしを構成する)
  - リソースをスケールする  
    (Scale resources)
  - データ パイプラインのテストを作成する  
    (Create tests for data pipelines)
  - 分析/トランザクションの目的に対するパイプラインを最適化する  
    (Optimize pipelines for analytical or transactional purposes)
  - 割り込みをハンドリングする  
    (Handle interruptions)
  - 例外ハンドリングを構成する  
    (Configure exception handling)
  - データを upsert する  
    (Upsert data)
  - アーカイブ ストリーム データを再生する  
    (Replay archived stream data)
- バッチとパイプラインを管理する  
  (Manage batches and pipelines)
  - バッチをトリガーする  
    (Trigger batches)
  - 失敗したバッチ読み込みをハンドリングする  
    (Handle failed batch loads)
  - バッチ読み込みを検証する  
    (Validate batch loads)
  - Azure Data Factory/Azure Synapse Pipelines のデータ パイプラインを管理する  
    (Manage data pipelines in Azure Data Factory or Azure Synapse Pipelines)
  - Azure Data Factory/Azure Synapse Pipelines のデータ パイプラインをスケジュールする  
    (Schedule data pipelines in Data Factory or Azure Synapse Pipelines)
  - パイプライン アーティファクトのバージョン制御を実装する  
    (Implement version control for pipeline artifacts)
  - パイプラインの Spark ジョブを管理する  
    (Manage Spark jobs in a pipeline)
#### データ ストレージおよびデータ処理のセキュリティ保護、監視、最適化 (Secure, monitor, and optimize data storage and data processing) (30-35%)
- データ セキュリティを実装する  
  (Implement data security)
  - データ マスクを実装する  
    (Implement data masking)
  - 保存中と移動中のデータを暗号化する  
    (Encrypt data at rest and in motion)
  - 行レベルと列レベル セキュリティを実装する  
    (Implement row-level and column-level security)
  - Azure ロール ベースのアクセス制御 (RBAC) を実装する  
    (Implement Azure role-based access control (RBAC))
  - Data Lake Storage Gen2 の POSIX ライクな アクセス制御リスト (ACL) を実装する  
    (Implement POSIX-like access control lists (ACLs) for Data Lake Storage Gen2)
  - データ保持ポリシーを実装する  
    (Implement a data retention policy)
  - セキュアなエンドポイント (プライベートとパブリック) を実装する  
    (Implement secure endpoints (private and public))
  - Azure Databricks のリソース トークンを実装する  
    (Implement resource tokens in Azure Databricks)
  - 機密情報を持つ DataFrame を読み込む  
    (Load a DataFrame with sensitive information)
  - テーブル/Parquet ファイルに暗号化データを書き込む  
    (Write encrypted data to tables or Parquet files)
  - 機密情報を管理する  
    (Manage sensitive information)
- データ ストレージとデータ処理を監視する  
  (Monitor data storage and data processing)
  - Azure Monitor を用いてロギングを実装する  
    (Implement logging used by Azure Monitor)
  - モニタリング サービスを構成する  
    (Configure monitoring services)
  - ストリーム 処理を監視する  
    (Monitor stream processing)
  - データ移動のパフォーマンスを測定する  
    (Measure performance of data movement)
  - システム全体のデータに関する統計情報を監視、更新する  
    (Monitor and update statistics about data across a system)
  - データ パイプライン パフォーマンスを監視する  
    (Monitor data pipeline performance)
  - クエリ パフォーマンスを計測する  
    (Measure query performance)
  - パイプライン テストをスケジュール、監視する  
    (Schedule and monitor pipeline tests)
  - Azure Monitor メトリックとログを扱う  
    (Interpret Azure Monitor metrics and logs)
  - パイプライン アラート戦略を実装する  
    (Implement a pipeline alert strategy)
- データ ストレージとデータ処理を最適化、トラブルシュートする  
  (Optimize and troubleshoot data storage and data processing)
  - 小さなファイルにコンパクト化する  
    (Compact small files)
  - データの偏りをハンドリングする  
    (Handle skew in data)
  - データ流出をハンドリングする  
    (Handle data spill)
  - リソース管理を最適化する  
    (Optimize resource management)
  - インデクサーを用いて、クエリをチューニングする  
    (Tune queries by using indexers)
  - キャッシュを用いて、クエリをチューニングする  
    (Tune queries by using cache)
  - 失敗した Spark ジョブをトラブルシュートする  
    (Troubleshoot a failed Spark job)
  - 外部サービスで実行されたアクティビティを含む、失敗したパイプライン実行をトラブルシュートする  
    (Troubleshoot a failed pipeline run, including activities executed in external services)

## 関連するラーニング パス (Microsoft Learn)
- Azure でのデータ エンジニアリングの概要 - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/get-started-data-engineering/
- Azure Synapse サーバーレス SQL プールを使用して Data Analytics ソリューションを構築する - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/build-data-analytics-solutions-using-azure-synapse-serverless-sql-pools/
- Azure Synapse Apache Spark プールで Data Engineering を実行する - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/perform-data-engineering-with-azure-synapse-apache-spark-pools/
- Azure Synapse Analytics を使用してデータウェア ハウスを操作する - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/work-with-data-warehouses-using-azure-synapse-analytics/
- Azure Synapse Analytics パイプラインを使用してデータを転送および変換する - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/transfer-transform-data-azure-synapse-analytics-pipelines/
- Azure Synapse Analytics を使用したハイブリッド トランザクションおよび分析処理ソリューションの操作 - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/work-with-hybrid-transactional-analytical-processing-solutions/
- Azure Stream Analytics を使用してデータ ストリーミング ソリューションを実装する - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/implement-data-streaming-with-asa/
- エンタープライズ全体でデータを管理する - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/govern-data-across-enterprise/
- Azure Databricks を使用した Data Engineering - Training | Microsoft Learn  
https://learn.microsoft.com/training/paths/data-engineer-azure-databricks/

## 関連サイト
- Microsoft Certified: Azure Data Engineer Associate - Learn | Microsoft Docs  
https://docs.microsoft.com/ja-jp/learn/certifications/azure-data-engineer
